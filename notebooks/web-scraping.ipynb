{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsibility_synonyms = [\"You Have\",\"Required Skills\",\"WHAT WE'RE LOOKING FOR\", 'key responsibilities',\"Who You Are\" \"What you bring:\", \"Who are we looking for\",    \"Required Competencies & Qualifications:\",\n",
    "    \"WHAT WE'RE LOOKING FOR:\",\"Core Responsibilities\",\"What you'll need\",\n",
    "    \"We'd love to hear from you if you:\",\"We want you if you are\", \"About You\", \n",
    "    \"About You:\",\"What You Have\",\n",
    "    \"We're Excited About You Because:\",\"Minimum qualifications:\",\"The Ideal Candidate/Bonus Points\",\n",
    "    \"Qualifications\",\n",
    "    \"Who Are We Looking For\",     \"We want you if you are:\",\n",
    "    \"Required Competencies & Qualifications:\",\"What you need to succeed\",\n",
    "    \"WHAT WE'RE LOOKING FOR:\",\n",
    "    \"We'd love to hear from you if you:\",\"We prefer a teammate with:\",\"Requirements\",\n",
    "    \"About You:\",\n",
    "    \"We're Excited About You Because:\",\"Skills You'll Need to Bring:\",\"Skills and Qualifications\",\n",
    "    \"Qualifications:\",\n",
    "    \"Who Are We Looking For:\",\n",
    "    \"basic qualifications:\", \"We would like to see:\", \"Preferred Qualifications\", \"What is required for you to be successful in this role\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_excel('../data/Job Description Links.xlsx')\n",
    "links2 = pd.read_csv('../data/url_links_productsthatcount.csv')\n",
    "links3 = pd.read_csv('../data/url_links_builtinnyc.csv')\n",
    "links4 = pd.read_csv('../data/url_links_builtinnyc2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links5 = pd.read_csv('../data/productthatcount_amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links5.drop(columns = 'Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links.dropna()\n",
    "#links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2418469/princi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2418469/princi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://jobs.productsthatcount.com/companies/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://jobs.productsthatcount.com/companies/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jobs.productsthatcount.com/companies/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2410591/princi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2410440/sr-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2410440/sr-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2383837/sr-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>https://www.amazon.jobs/en/jobs/2383837/sr-pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Links\n",
       "0    https://www.amazon.jobs/en/jobs/2418469/princi...\n",
       "1    https://www.amazon.jobs/en/jobs/2418469/princi...\n",
       "2    https://jobs.productsthatcount.com/companies/a...\n",
       "3    https://jobs.productsthatcount.com/companies/a...\n",
       "4    https://jobs.productsthatcount.com/companies/a...\n",
       "..                                                 ...\n",
       "795  https://www.amazon.jobs/en/jobs/2410591/princi...\n",
       "796  https://www.amazon.jobs/en/jobs/2410440/sr-pro...\n",
       "797  https://www.amazon.jobs/en/jobs/2410440/sr-pro...\n",
       "798  https://www.amazon.jobs/en/jobs/2383837/sr-pro...\n",
       "799  https://www.amazon.jobs/en/jobs/2383837/sr-pro...\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_keywords_from_paragraph(div):\n",
    "    #print(div)\n",
    "\n",
    "    paragraph_text = div.get_text()\n",
    "    processed_paragraph_text = re.sub(r'[^a-zA-Z0-9\\s]', '', paragraph_text.lower())\n",
    "    for word in responsibility_synonyms:\n",
    "        processed_word = re.sub(r'[^a-zA-Z0-9\\s]', '', word.lower())\n",
    "        #print(processed_word)\n",
    "        if processed_word in processed_paragraph_text:\n",
    "\n",
    "            # container = div.find_parent(['div', 'header', 'h1', 'h2', 'h3', 'h4', 'h5', 'p'])\n",
    "            # if container:\n",
    "                \n",
    "            content = div.find_next_sibling([\"ul\", \"p\"])\n",
    "            if content:\n",
    "                print(\"Found word\")\n",
    "                #print(\"conten\", content)\n",
    "                if content.name == \"ul\":\n",
    "                    terms = [li.get_text() for li in content.find_all(\"li\")]\n",
    "                    return word, terms\n",
    "                else:\n",
    "                    return word, content.get_text()\n",
    "                #paragraph_element = div.find_next_sibling([\"p\"])\n",
    "                # if ul_element:\n",
    "                #     print(\"list element\")\n",
    "                #     li_elements = ul_element.find_all(\"li\")\n",
    "                #     return word, li_elements\n",
    "                # elif paragraph_element:\n",
    "                #     print(\"paragraph element\")\n",
    "                #     return word, paragraph_element.get_text()\n",
    "            #else:\n",
    "            #     return word, processed_paragraph_text\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import time\n",
    "\n",
    "# from timeout_decorator import timeout\n",
    "\n",
    "# @timeout(70)  # Set the timeout to 10 seconds\n",
    "\n",
    "def go_through_tags(div_elements):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if div_elements:\n",
    "            for id, div_element in enumerate(div_elements):\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                if elapsed_time > 30:  # Replace with your threshold\n",
    "                    print(\"Processing took too long. Stopping loop.\")\n",
    "                    break  # Exit the loop\n",
    "\n",
    "                if div_element and isinstance(div_element, Tag):\n",
    "                    ul_terms = find_keywords_from_paragraph(div_element)\n",
    "                    if ul_terms:\n",
    "                        return ul_terms\n",
    "                    else:\n",
    "                        div_within_div = div_element.find_all([\"div\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"p\"])\n",
    "                        #print(\"Entering this also\")\n",
    "                        for div in div_within_div:\n",
    "                            result = go_through_tags(div)\n",
    "                            if result:\n",
    "                                return result\n",
    "    except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            #else: None, None, \"Div not found\"\n",
    "\n",
    "    # else:\n",
    "    #     return None, None, \"Keyword not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links4['Links'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found word\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:05,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found word\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['url', 'keyword', 'terms', 'error', 'access_date'])\n",
    "\n",
    "# URL of the page to scrape\n",
    "\n",
    "url_list = links4['Links'].unique()[230:240]\n",
    "    \n",
    "for idx, url in tqdm(enumerate(url_list)):\n",
    "\n",
    "    df.at[idx, 'url'] = url\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # # Parse the HTML content using Beautiful Soup\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            div_elements = soup.find_all(\"div\")\n",
    "\n",
    "            ul = go_through_tags(div_elements)\n",
    "            df.at[idx, 'terms'] = ul[1]\n",
    "            df.at[idx, 'keyword'] = ul[0]\n",
    "            df.at[idx, 'access_date'] = datetime.today()\n",
    "                \n",
    "\n",
    "        else:\n",
    "            df.at[idx, 'error'] = 'Page is unavailable'\n",
    "    # except requests.Timeout:\n",
    "    #     df.at[idx, 'error'] = 'Request timed out'\n",
    "    except Exception as e:\n",
    "        df.at[idx, 'error'] = e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>keyword</th>\n",
       "      <th>terms</th>\n",
       "      <th>error</th>\n",
       "      <th>access_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.builtinnyc.com/job/product/lead-pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url keyword terms error  \\\n",
       "0  https://www.builtinnyc.com/job/product/lead-pr...     NaN   NaN   NaN   \n",
       "\n",
       "  access_date  \n",
       "0         NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = df.dropna(subset='terms')\n",
    "amazon_df = pd.concat([amazon_df, df.dropna(subset='terms')])\n",
    "amazon_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.dropna(subset='terms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_df, df.dropna(subset='terms')])\n",
    "\n",
    "#.to_csv('../data/requirements_sheet_jd_buuiltinny4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset='terms').to_csv('../data/requirements_sheet_jd_productthatcount1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../data/requirements_sheet_jd_buuiltinny6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.to_csv('../data/requirements_sheet_jd_productthatcount_amazon.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-description-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
